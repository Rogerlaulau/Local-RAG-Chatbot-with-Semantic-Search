### âœ… `README.md`

````markdown
# ğŸ’¬ Local RAG Chatbot with LLM Inference via OpenRouter

A local-first Retrieval-Augmented Generation (RAG) chatbot that lets users upload `.txt`, `.pdf`, and `.html` files to query their content using natural language. It uses **Chroma** as the vector store, **OpenAI Embedding API** for semantic search, and routes inference to **OpenRouter.ai** to leverage powerful open-source LLMs.

> âš¡ Designed to run locally on a MacBook (M1, 16GB RAM).

---

## ğŸ§  Architecture Overview

```mermaid
flowchart TD
    A[User Uploads Files] --> B[Text Extraction (.txt/.pdf/.html)]
    B --> C[Embedding via OpenAI API]
    C --> D[Store Vectors in Chroma DB (local)]
    E[User Query] --> F[Embed Query via OpenAI API]
    F --> G[Retrieve Relevant Chunks from Chroma]
    G --> H[Send Context + Query to LLM via OpenRouter.ai]
    H --> I[LLM Response Returned to User]
````

---

## ğŸ“¦ Features

* Accepts `.txt`, `.pdf`, `.html` file uploads.
* Embeds documents and queries using OpenAI embeddings.
* Retrieves top-k relevant chunks using ChromaDB.
* Sends prompt + context to OpenRouter-compatible models.
* Local, lightweight, and developer-friendly.

---

## ğŸš€ Getting Started

### 1. **Clone the repo**

```bash
git clone https://github.com/Rogerlaulau/Local-RAG-Chatbot-with-Semantic-Search.git
cd Local-RAG-Chatbot-with-Semantic-Search
```

### 2. **Set up your Python environment**

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 3. **Environment Variables**

Create a `.env` file in the root directory with the following:

```env
OPENROUTER_API_KEY=your-openrouter-api-key
CHROMA_DB_PATH=./chroma_db
```

### 4. **Run the Service**

```bash
python3 ingest_documents.py ./context/sample.pdf
python3 rag_chat.py
```
OR
```
streamlit run streamlit_app.py
```
---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ ingest_documents.py     # Handles .txt, .pdf, .html parsing and embed documents into ChromaDB
â”œâ”€â”€ rag_chat.py             # Top-K vector search
â”œâ”€â”€ streamlit_app.py        # Web UI for interaction
â”œâ”€â”€ context                 # Input documents
â”œâ”€â”€ requirements.txt        # Required packages
â””â”€â”€ .env                    # API keys and configs
```

---

## ğŸ§  Example Use Case

> Upload a research paper PDF or a set of HTML documentation files. Ask questions like:
>
> * â€œWhat is the main conclusion of this paper?â€
> * â€œList all mentioned APIs from the docs.â€
> * â€œSummarize the key risks discussed.â€

---

## ğŸ”§ Tech Stack

| Component         | Tool/Library                           |
| ----------------- | -------------------------------------- |
| Vector DB         | [Chroma](https://www.trychroma.com/)   |
| Embeddings        | OpenAI Embeddings API                  |
| LLM Inference     | [OpenRouter.ai](https://openrouter.ai) |
| Backend Framework | Flask or FastAPI                       |
| File Parsing      | PyMuPDF, BeautifulSoup                 |
| Deployment        | Localhost / MacBook M1                 |

---

## ğŸ“Œ Future Enhancements

* Local LLM fallback (e.g. llama.cpp or Ollama)
* Streamed responses
* Frontend interface (React/Tailwind)
* Support for `.docx` and `.md`

---

## ğŸ›¡ï¸ Disclaimer

This project is for educational and prototyping purposes. API keys are required to interact with external services. Always handle sensitive information securely.

---

## ğŸ§‘â€ğŸ’» Author

**Roger L.** â€“ 
Passionate about AI, LLM integration, and building impactful developer tools.

---

## ğŸ“„ License

MIT License. See `LICENSE` file for details.
